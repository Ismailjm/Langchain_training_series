{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki = WikipediaQueryRun(api_wrapper=wiki_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000021B7DFD5150>, search_kwargs={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.1/docs/modules/tools/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "vectorDb = FAISS.from_documents(documents, GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "retriever = vectorDb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(retriever, \"langchain_tools_search\",\n",
    "                      \"Search for informations about tools in langsmith. For any question about tools u must use this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They combine a few things:The name of the toolA description of what the tool isJSON schema of what the inputs to the tool areThe function to call Whether the result of a tool should be returned directly to the userIt is useful to have all this information because this information can be used to build action-taking systems! The name, description, and JSON schema can be used to prompt the LLM so it knows how to specify what action to take, and then the function to call is equivalent to taking that action.The simpler the input to a tool is, the easier it is for an LLM to be able to use it.\\nMany agents will only work with tools that have a single string input.\\n\\nSkip to main contentA newer LangChain version is out! Check out the latest version.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsToolkitsToolsDefining Custom ToolsTools as OpenAI FunctionsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).CompositionToolsOn this pageToolsTools are interfaces that an agent, chain, or LLM can use to interact with the world.\\n\\nof tools that work well together. For a more in depth description as well as a list of all built-in toolkits, see this pageTools as OpenAI Functions: Tools are very similar to OpenAI Functions, and can easily be converted to that format. See this notebook for instructions on how to do that.Help us out by providing feedback on this documentation page:PreviousCompositionNextToolkitsDefault ToolsCustomizing Default ToolsMore TopicsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\nFor a list of agent types and which ones work with more complicated inputs, please see this documentationImportantly, the name, description, and JSON schema (if used) are all used in the prompt. Therefore, it is really important that they are clear and describe exactly how the tool should be used. You may need to change the default name, description, or JSON schema if the LLM is not understanding how to use the tool.Default Tools\\u200bLet's take a look at how to work with tools. To do this, we'll work with a built in tool.from langchain_community.tools import WikipediaQueryRunfrom langchain_community.utilities import WikipediaAPIWrapperAPI Reference:WikipediaQueryRunWikipediaAPIWrapperNow we initialize the tool. This is where we can configure it as we pleaseapi_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)tool = WikipediaQueryRun(api_wrapper=api_wrapper)This is the default nametool.name'Wikipedia'This is the default descriptiontool.description'A wrapper around\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.name\n",
    "retriever_tool.invoke(\"what are tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arxiv tool\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name\n",
    "arxiv.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki, arxiv,retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\") # this solved the problem of the project not appearing in langsmith dashboard\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3.1')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "## Load Ollama LAMA2 LLM model\n",
    "llm=ChatOllama(model=\"llama3.1\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "agent = create_openai_tools_agent(llm,tools,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000021B6BE6C2C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000021B6BE6C2C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatOllama(model='llama3.1'), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langchain_tools_search', 'description': 'Search for informations about tools in langsmith. For any question about tools u must use this.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\dexte\\\\OneDrive\\\\Documents\\\\GitHub\\\\Langchain_tuto\\\\venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)), Tool(name='langchain_tools_search', description='Search for informations about tools in langsmith. For any question about tools u must use this.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000021B6C7E7100>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000021B7DFD5150>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000021B6CB95300>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000021B7DFD5150>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agent executor\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent,tools=tools, verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangChain is an open-source framework for building conversational AI models, and it comes with a wide range of pre-built tools that can be used to enhance the functionality of your chatbots. Here are some of the key tools available in LangChain:\n",
      "\n",
      "1. **LLaMA**: This is a large language model (LLM) inference tool that allows you to use pre-trained LLMs within your LangChain models. LLaMA provides a flexible interface for integrating various LLMs, making it easier to experiment with different models.\n",
      "2. **LLaMA-Chat**: A simple chatbot tool built on top of the LLaMA model. This tool enables you to build conversational interfaces using pre-trained language models and can be used as a starting point for more complex applications.\n",
      "3. **Chain**: This is the core tool in LangChain, responsible for building and managing conversation flows. Chain provides a graph-based API that allows you to create custom chatbot experiences by linking together various components, such as intents, entities, and responses.\n",
      "4. **Prompt-Tools**: These are pre-built tools for generating and manipulating input prompts for your chatbots. Prompt-Tools include features like automatic prompt completion, entity extraction, and contextual understanding.\n",
      "5. **Dialogue-Manager**: This tool enables you to manage conversational states and flow between different dialogue components within a single conversation.\n",
      "6. **Intent-Classifier**: A machine learning-based intent classification tool that helps identify the user's intentions or goals based on their input.\n",
      "7. **Entity-Extractor**: A pre-trained model for extracting relevant entities (e.g., names, dates, locations) from user input, which can be used to trigger specific dialogue flows.\n",
      "\n",
      "These tools can be combined in various ways to build custom chatbot applications with advanced features like:\n",
      "\n",
      "* Contextual understanding and memory\n",
      "* Entity-based dialogue flows\n",
      "* Intent-based routing and handling\n",
      "* Customizable conversational interfaces\n",
      "\n",
      "By leveraging these tools within the LangChain framework, you can create sophisticated chatbots that provide a more natural and engaging experience for users.\n",
      "\n",
      "How would you like to proceed? Would you like me to elaborate on any of these tools or demonstrate an example use case?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about tools in langchain',\n",
       " 'output': \"LangChain is an open-source framework for building conversational AI models, and it comes with a wide range of pre-built tools that can be used to enhance the functionality of your chatbots. Here are some of the key tools available in LangChain:\\n\\n1. **LLaMA**: This is a large language model (LLM) inference tool that allows you to use pre-trained LLMs within your LangChain models. LLaMA provides a flexible interface for integrating various LLMs, making it easier to experiment with different models.\\n2. **LLaMA-Chat**: A simple chatbot tool built on top of the LLaMA model. This tool enables you to build conversational interfaces using pre-trained language models and can be used as a starting point for more complex applications.\\n3. **Chain**: This is the core tool in LangChain, responsible for building and managing conversation flows. Chain provides a graph-based API that allows you to create custom chatbot experiences by linking together various components, such as intents, entities, and responses.\\n4. **Prompt-Tools**: These are pre-built tools for generating and manipulating input prompts for your chatbots. Prompt-Tools include features like automatic prompt completion, entity extraction, and contextual understanding.\\n5. **Dialogue-Manager**: This tool enables you to manage conversational states and flow between different dialogue components within a single conversation.\\n6. **Intent-Classifier**: A machine learning-based intent classification tool that helps identify the user's intentions or goals based on their input.\\n7. **Entity-Extractor**: A pre-trained model for extracting relevant entities (e.g., names, dates, locations) from user input, which can be used to trigger specific dialogue flows.\\n\\nThese tools can be combined in various ways to build custom chatbot applications with advanced features like:\\n\\n* Contextual understanding and memory\\n* Entity-based dialogue flows\\n* Intent-based routing and handling\\n* Customizable conversational interfaces\\n\\nBy leveraging these tools within the LangChain framework, you can create sophisticated chatbots that provide a more natural and engaging experience for users.\\n\\nHow would you like to proceed? Would you like me to elaborate on any of these tools or demonstrate an example use case?\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about tools in langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mReID (Re-Identification) is a subfield of computer vision that focuses on identifying the same individual across non-overlapping camera views or images. The goal of ReID is to associate objects, specifically people, between different cameras or scenes, even if they are not directly connected.\n",
      "\n",
      "In the context of surveillance and security applications, ReID aims to help identify individuals who may be recognized from a specific camera view in another part of the system or across multiple cameras. This can be particularly useful for tracking suspects, identifying known individuals, or monitoring crowd behavior.\n",
      "\n",
      "The tasks involved in ReID include:\n",
      "\n",
      "1. **Person detection**: Locating individuals within images or videos.\n",
      "2. **Person re-identification**: Associating individuals between non-overlapping camera views or images.\n",
      "3. **Person tracking**: Following an individual across multiple cameras or scenes.\n",
      "\n",
      "ReID is a challenging problem due to variations in lighting, pose, occlusion, and appearance between different camera views. To overcome these challenges, ReID methods typically use:\n",
      "\n",
      "1. **Deep learning-based approaches**: Convolutional Neural Networks (CNNs) are commonly used for feature extraction and classification.\n",
      "2. **Metric learning**: Techniques like Triplet Loss or Contrastive Loss are employed to learn a metric that captures the similarity between images of the same individual.\n",
      "3. **Generative models**: Some ReID methods use generative models, such as Generative Adversarial Networks (GANs), to generate synthetic data and improve model robustness.\n",
      "\n",
      "Some popular benchmarks for evaluating ReID performance include:\n",
      "\n",
      "1. **Market-1501**: A large-scale dataset with 1501 identities, commonly used for evaluating person re-identification.\n",
      "2. **MARS**: Another large-scale dataset with 1253 identities, which focuses on pedestrian tracking in surveillance scenarios.\n",
      "3. **DUKE-MTMC**: A multi-camera benchmark that combines pedestrian detection and re-identification.\n",
      "\n",
      "ReID has many practical applications in various domains, such as:\n",
      "\n",
      "1. **Surveillance**: ReID can be used to track suspects or identify known individuals across multiple cameras.\n",
      "2. **Smart cities**: ReID can help monitor crowd behavior and detect anomalies in public spaces.\n",
      "3. **Retail**: ReID can aid in identifying customers and improving personalized marketing strategies.\n",
      "\n",
      "Overall, ReiD is an essential area of research in computer vision that has the potential to improve security, surveillance, and other applications where person identification across multiple cameras or scenes is crucial.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about ReiID in computer vision',\n",
       " 'output': 'ReID (Re-Identification) is a subfield of computer vision that focuses on identifying the same individual across non-overlapping camera views or images. The goal of ReID is to associate objects, specifically people, between different cameras or scenes, even if they are not directly connected.\\n\\nIn the context of surveillance and security applications, ReID aims to help identify individuals who may be recognized from a specific camera view in another part of the system or across multiple cameras. This can be particularly useful for tracking suspects, identifying known individuals, or monitoring crowd behavior.\\n\\nThe tasks involved in ReID include:\\n\\n1. **Person detection**: Locating individuals within images or videos.\\n2. **Person re-identification**: Associating individuals between non-overlapping camera views or images.\\n3. **Person tracking**: Following an individual across multiple cameras or scenes.\\n\\nReID is a challenging problem due to variations in lighting, pose, occlusion, and appearance between different camera views. To overcome these challenges, ReID methods typically use:\\n\\n1. **Deep learning-based approaches**: Convolutional Neural Networks (CNNs) are commonly used for feature extraction and classification.\\n2. **Metric learning**: Techniques like Triplet Loss or Contrastive Loss are employed to learn a metric that captures the similarity between images of the same individual.\\n3. **Generative models**: Some ReID methods use generative models, such as Generative Adversarial Networks (GANs), to generate synthetic data and improve model robustness.\\n\\nSome popular benchmarks for evaluating ReID performance include:\\n\\n1. **Market-1501**: A large-scale dataset with 1501 identities, commonly used for evaluating person re-identification.\\n2. **MARS**: Another large-scale dataset with 1253 identities, which focuses on pedestrian tracking in surveillance scenarios.\\n3. **DUKE-MTMC**: A multi-camera benchmark that combines pedestrian detection and re-identification.\\n\\nReID has many practical applications in various domains, such as:\\n\\n1. **Surveillance**: ReID can be used to track suspects or identify known individuals across multiple cameras.\\n2. **Smart cities**: ReID can help monitor crowd behavior and detect anomalies in public spaces.\\n3. **Retail**: ReID can aid in identifying customers and improving personalized marketing strategies.\\n\\nOverall, ReiD is an essential area of research in computer vision that has the potential to improve security, surveillance, and other applications where person identification across multiple cameras or scenes is crucial.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about ReiID in computer vision\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
